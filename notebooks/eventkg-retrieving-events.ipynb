{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EventKG - Retrieving events for experiments\n",
    "\n",
    "The aim of the notebook is to automatically retrieve events for the search experiment, with other information like start and end dates.\n",
    "\n",
    "Before running the notebook, ensure to have the followings:\n",
    "* EventKG downloaded and preprocessed, cf. `eventkg-filtering.ipynb`\n",
    "* Subset of EventKG loaded in [GraphDB](https://graphdb.ontotext.com)\n",
    "* GraphDB endpoint active (Repositories name `eventkg`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import requests\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "import ray\n",
    "from ray.util.multiprocessing.pool import Pool\n",
    "import pandas as pd\n",
    "from settings import FOLDER_PATH\n",
    "from src.hdt_interface import HDTInterface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Entering and loading variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TO-DO: change if necessary>\n",
    "FOLDER_SAVE_DATA = os.path.join(FOLDER_PATH, \"data-all\")\n",
    "DATASET = \"yago\"\n",
    "\n",
    "DATASET_TO_FOLDER = {\n",
    "    \"wikidata\": \"wikidata-2021-03-05\",\n",
    "    \"dbpedia\": \"dbpedia-snapshot-2021-09\",\n",
    "    \"yago\": \"yago-2020-02-24\"\n",
    "}\n",
    "\n",
    "nested_dataset = 0 if DATASET == \"wikidata\" else 1\n",
    "filter_kb = 1 if DATASET == \"dbpedia\" else 0\n",
    "\n",
    "ENDPOINT = \"http://localhost:7200/repositories/eventkg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TO_START_URI = {\n",
    "    \"dbpedia\": \"http://dbpedia\",\n",
    "    \"wikidata\": \"http://www.wikidata\",\n",
    "    \"yago\": \"http://yago\"\n",
    "}\n",
    "\n",
    "HEADERS = {\n",
    "    \"Accept\": \"text/csv\"\n",
    "}\n",
    "\n",
    "NB_CPUS = psutil.cpu_count(logical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating folder if necessary\n",
    "if not os.path.exists(FOLDER_SAVE_DATA):\n",
    "    os.makedirs(FOLDER_SAVE_DATA)\n",
    "if not os.path.exists(os.path.join(FOLDER_SAVE_DATA, DATASET)):\n",
    "    os.makedirs(os.path.join(FOLDER_SAVE_DATA, DATASET))\n",
    "for folder in [\"config\", \"gs_events\", \"referents\", \"other\"]:\n",
    "    path_folder = os.path.join(FOLDER_SAVE_DATA, DATASET, folder)\n",
    "    if not os.path.exists(path_folder):\n",
    "        os.makedirs(path_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading params for search\n",
    "with open(os.path.join(FOLDER_PATH, \"dataset-config\", f\"{DATASET}.yaml\"),\n",
    "          encoding='utf-8') as file:\n",
    "    dataset_config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "interface = HDTInterface(dataset_config=dataset_config, default_pred=[],\n",
    "                         folder_hdt=DATASET_TO_FOLDER[DATASET],\n",
    "                         nested_dataset=nested_dataset,\n",
    "                         filter_kb=filter_kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retrieving events with the most sub events\n",
    "\n",
    "Using SPARQL Query + GraphDB endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_RETRIEVE_EVENTS = \"\"\"\n",
    "PREFIX sem: <http://semanticweb.cs.vu.nl/2009/11/sem/>\n",
    "PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "SELECT ?eventKG (COUNT(DISTINCT ?subEventKG) as ?nbSubEvent)\n",
    "WHERE {\n",
    "    \n",
    " ?event sem:hasSubEvent* ?subEvent .\n",
    " ?event sem:hasBeginTimeStamp ?startTimeEvent .\n",
    " ?event sem:hasEndTimeStamp ?endTimeEvent .\n",
    " ?event owl:sameAs ?eventKG .\n",
    "\n",
    " ?subEvent owl:sameAs ?subEventKG .\n",
    " ?subEvent sem:hasBeginTimeStamp ?startTimeSubEvent .\n",
    " ?subEvent sem:hasEndTimeStamp ?endTimeSubEvent .\n",
    " \n",
    " FILTER (?endTimeSubEvent >= ?startTimeEvent) .\n",
    " FILTER (?startTimeSubEvent <= ?endTimeEvent) .\n",
    " FILTER( strStarts( str(?eventKG), \"<dataset_start_uri>\" ) ) .\n",
    " FILTER( strStarts( str(?subEventKG), \"<dataset_start_uri>\" ) ) .\n",
    "}\n",
    "GROUP BY ?eventKG\n",
    "ORDER BY DESC(?nbSubEvent)\n",
    "\"\"\"\n",
    "\n",
    "QUERY_RETRIEVE_EVENTS = \\\n",
    "    f\"{QUERY_RETRIEVE_EVENTS.replace('<dataset_start_uri>', DATASET_TO_START_URI[DATASET])}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(ENDPOINT, headers=HEADERS,\n",
    "                        params={\"query\": QUERY_RETRIEVE_EVENTS})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events = pd.read_csv(\n",
    "    io.StringIO(response.content.decode('utf-8'))\n",
    ")\n",
    "df_events.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieving info for each selected event\n",
    "\n",
    "* Ground truth events from EventKG \n",
    "* Referents (URI mapping)\n",
    "* Start/End dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Ground truth for each event\n",
    "\n",
    "Ground truth = event part of that event in EventKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(FOLDER_SAVE_DATA, DATASET, \"other\", \"events_sub_events.csv\")\n",
    "df_events.to_csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events = pd.read_csv(save_path)\n",
    "df_events = df_events[[col for col in df_events.columns if col != \"Unnamed: 0\"]]\n",
    "\n",
    "print(f\"# of events: {df_events.shape[0]}\")\n",
    "print(f\"# of events with more than 10 sub events: {df_events[df_events.nbSubEvent >= 10].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_GROUND_TRUTH_TEMPLATE = \"\"\"\n",
    "PREFIX sem: <http://semanticweb.cs.vu.nl/2009/11/sem/>\n",
    "PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "SELECT DISTINCT(?subEventKG as ?linkDBpediaEn)\n",
    "WHERE {\n",
    "    \n",
    "?event owl:sameAs <event-to-replace> .\n",
    "?event sem:hasSubEvent* ?subEvent .\n",
    "?subEvent owl:sameAs ?subEventKG .\n",
    "    \n",
    "?event sem:hasBeginTimeStamp ?startTimeEvent .\n",
    "?event sem:hasEndTimeStamp ?endTimeEvent .\n",
    "\n",
    "?subEvent sem:hasBeginTimeStamp ?startTimeSubEvent .\n",
    "?subEvent sem:hasEndTimeStamp ?endTimeSubEvent .\n",
    "    \n",
    "FILTER( strStarts( str(?subEventKG), \"<dataset_start_uri>\" ) ) .\n",
    "FILTER (?endTimeSubEvent >= ?startTimeEvent) .\n",
    "FILTER (?startTimeSubEvent <= ?endTimeEvent) .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "QUERY_GROUND_TRUTH_TEMPLATE = QUERY_GROUND_TRUTH_TEMPLATE.replace('<dataset_start_uri>', DATASET_TO_START_URI[DATASET])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = df_events[df_events.nbSubEvent >= 10].eventKG.values\n",
    "for i in tqdm(range(len(events))):\n",
    "    event = events[i]\n",
    "    name = event.split(\"/\")[-1]\n",
    "    query = QUERY_GROUND_TRUTH_TEMPLATE.replace(\n",
    "        \"event-to-replace\", event\n",
    "    )\n",
    "    response = requests.get(ENDPOINT, headers=HEADERS,\n",
    "                            params={\"query\": query})\n",
    "    pd.read_csv(io.StringIO(response.content.decode('utf-8'))) \\\n",
    "        .to_csv(os.path.join(FOLDER_SAVE_DATA, DATASET,\n",
    "                             \"gs_events\",\n",
    "                             f\"{name}.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. URI referents for each sub event - Only for DBpedia\n",
    "\n",
    "Due to differences in dataset version, URIs can vary over time, the aim of this section is to retrieve a unique ID referent for each set of URIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.get_equivalent_url import get_equivalent_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_equivalent_url(df_path, save_path, dataset):\n",
    "    if not os.path.exists(save_path):\n",
    "        get_equivalent_url(df_path=df_path, save_path=save_path, dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_folder = os.path.join(FOLDER_SAVE_DATA, DATASET, \"gs_events\")\n",
    "json_folder = os.path.join(FOLDER_SAVE_DATA, DATASET, \"referents\")\n",
    "\n",
    "csv_files = os.listdir(csv_folder)\n",
    "\n",
    "args = [\n",
    "    (os.path.join(csv_folder, csv_file),\n",
    "     os.path.join(\n",
    "         json_folder,\n",
    "         f\"{os.path.splitext(csv_file)[0]}.json\"),\n",
    "     DATASET) \\\n",
    "             for csv_file in csv_files\n",
    "]\n",
    "\n",
    "pool = Pool(processes=NB_CPUS)\n",
    "pool.starmap(add_equivalent_url, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Start and End dates of each event\n",
    "\n",
    "Minimum start date among all start dates, maximum end date among all end dates.\n",
    "\n",
    "Start date must be before end date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_DATES_TEMPLATE = \"\"\"\n",
    "PREFIX sem: <http://semanticweb.cs.vu.nl/2009/11/sem/>\n",
    "PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "SELECT (min(?startTimeEvent) as ?min) (max(?endTimeEvent) as ?max)\n",
    "WHERE {\n",
    "    \n",
    " ?event owl:sameAs <event-to-replace> .\n",
    " ?event sem:hasSubEvent* ?subEvent .\n",
    " ?event sem:hasBeginTimeStamp ?startTimeEvent .\n",
    " OPTIONAL { ?event sem:hasEndTimeStamp ?endTimeEvent . }\n",
    " ?event owl:sameAs ?eventKG .\n",
    "\n",
    " FILTER( strStarts( str(?eventKG), \"<dataset_start_uri>\" ) ) .\n",
    "}\n",
    "GROUP BY ?eventKG\n",
    "HAVING (max(?endTimeEvent) > min(?startTimeEvent))\n",
    "\"\"\"\n",
    "\n",
    "QUERY_DATES_TEMPLATE = QUERY_DATES_TEMPLATE.replace('<dataset_start_uri>', DATASET_TO_START_URI[DATASET])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = QUERY_DATES_TEMPLATE.replace(\n",
    "        \"event-to-replace\", \"http://yago-knowledge.org/resource/World_War_II\")\n",
    "response = requests.get(ENDPOINT, headers=HEADERS,\n",
    "                    params={\"query\": query})\n",
    "pd.read_csv(io.StringIO(response.content.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(event):\n",
    "    query = QUERY_DATES_TEMPLATE.replace(\n",
    "        \"event-to-replace\", event)\n",
    "    response = requests.get(ENDPOINT, headers=HEADERS,\n",
    "                        params={\"query\": query})\n",
    "    return pd.read_csv(io.StringIO(response.content.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ray.is_initialized() == True:\n",
    "    ray.shutdown()\n",
    "pool = Pool(processes=NB_CPUS)\n",
    "result = pool.map(get_dates, df_events[df_events.nbSubEvent >= 10].eventKG.values)\n",
    "if ray.is_initialized() == True:\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.triply_interface import TriplInterface\n",
    "\n",
    "interface = TriplInterface()\n",
    "\n",
    "def retrieve_date_triply(node):\n",
    "    predicate = \"http://dbpedia.org/ontology/startDate\"\n",
    "    triples = interface.run_request(params=dict(subject=node, predicate=predicate), filter_pred=[], filter_keep=False)\n",
    "    if len(triples) == 0:\n",
    "        return None, None\n",
    "    elif len(triples) == 1:\n",
    "        return str(triples[0][2]), str(triples[0][2])\n",
    "    else:\n",
    "        dates = [str(elt[2]) for elt in triples]\n",
    "        return min(dates), max(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_dates = {\n",
    "    \"2014_United_States_elections\": {\"start\": \"2014-11-04\", \"end\": \"2014-11-04\"},\n",
    "    \"2018_United_States_elections\": {\"start\": \"2018-11-06\", \"end\": \"2018-11-06\"},\n",
    "    \"Arab–Israeli_conflict\": {\"start\": \"1948-05-15\", \"end\": \"2021-12-31\"},\n",
    "    \"War_on_terror\": {\"start\": \"2011-09-11\", \"end\": \"2021-12-31\"},\n",
    "    \"Iraqi_conflict_(2003–present)\": {\"start\": \"2003-03-20\", \"end\": \"2021-12-31\"},\n",
    "    \"War_on_Terror\": {\"start\": \"2001-09-15\", \"end\": \"2022-12-31\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_changing_config(dico):\n",
    "    counter = 0\n",
    "    for i, event in enumerate(df_events[df_events.nbSubEvent >= 10].eventKG.values):\n",
    "        curr_df = result[i]\n",
    "        name = event.split(\"/\")[-1]\n",
    "        start, end = None, None\n",
    "        if curr_df.shape[0] != 0:\n",
    "            start = curr_df[\"min\"].values[0]\n",
    "            end = curr_df[\"max\"].values[0]\n",
    "        else:\n",
    "            #start, end = retrieve_date_triply(node=event)\n",
    "            if not (start and end) and name in manual_dates:\n",
    "                start, end = manual_dates.get(name).get(\"start\"), manual_dates.get(name).get(\"end\")\n",
    "        if start and end:\n",
    "            dico[event] = {\n",
    "                \"start\": event,\n",
    "                \"start_date\": start,\n",
    "                \"end_date\": end,\n",
    "                #\"gold_standard\": os.path.join(FOLDER_SAVE_DATA, \"gs_events\", f\"{name}.csv\"),\n",
    "                #\"referents\": os.path.join(FOLDER_SAVE_DATA, \"referents\", f\"{name}.json\"),\n",
    "                \"gold_standard\": f\"{name}.csv\",\n",
    "                \"referents\": f\"{name}.json\",\n",
    "                \"name_exp\": name,\n",
    "            }\n",
    "        else:\n",
    "            counter += 1\n",
    "            print(f\"Dates for {name} could not be found\")\n",
    "    print(f\"{counter} events could not be processed further\")\n",
    "    return dico\n",
    "\n",
    "dico_config = store_changing_config(dico={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\n",
    "    FOLDER_PATH, \"configs-example\", f\"config-{DATASET}.json\"), \"r\", encoding=\"utf-8\") as openfile:\n",
    "    BASE_CONFIG = json.load(openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in df_events[df_events.nbSubEvent >= 10].eventKG.values:\n",
    "    name = event.split(\"/\")[-1]\n",
    "    if event in dico_config:\n",
    "        BASE_CONFIG.update(dico_config[event])\n",
    "        with open(os.path.join(FOLDER_SAVE_DATA, DATASET, \"config\", f\"{name}.json\"), \"w\", encoding='utf-8') as openfile:\n",
    "            json.dump(BASE_CONFIG, openfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "afb6971e04c6e62d7bdf87aae553a4c9fea1ded343f2e99e4ce796810d312f94"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('graph_search_framework')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
